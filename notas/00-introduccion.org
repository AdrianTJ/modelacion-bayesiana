#+TITLE: Modelación Bayesiana
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: Primavera, 2022
:REVEAL_PROPERTIES:
#+LANGUAGE: es
#+OPTIONS: num:nil toc:nil timestamp:nil
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_THEME: night
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Modelación Bayesiana">
#+REVEAL_INIT_OPTIONS: width:1600, height:900, margin:.2
#+REVEAL_EXTRA_CSS: ./mods.css
#+REVEAL_PLUGINS: (notes)
:END:
#+STARTUP: showall
#+PROPERTY: header-args:R :session intro :exports both :results output org :tangle ../rscripts/intro.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc

#+BEGIN_NOTES

*Objetivo*. Repasar notación que utilizaremos a lo largo del curso. Establecer la motivación de los temas que trataremos en la materia. 

#+END_NOTES

* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#notación][Notación]]
  - [[#variables-aleatorias][Variables aleatorias]]
  - [[#distribución-paramétrica][Distribución paramétrica]]
  - [[#valores-esperados][Valores esperados]]
    - [[#abuso-de-notación][Abuso de notación]]
  - [[#estadísticas-de-interés][Estadísticas de interés]]
  - [[#probabilidad-condicional][Probabilidad condicional]]
- [[#repaso][Repaso]]
  - [[#regla-de-bayes][Regla de Bayes]]
  - [[#ejemplos][Ejemplos]]
- [[#motivación][Motivación]]
  - [[#distinción-importante][Distinción importante]]
  - [[#por-qué-necesitamos-un-flujo-de-trabajo][¿Por qué necesitamos un flujo de trabajo?]]
  - [[#proceso-iterativo][Proceso iterativo]]
:END:




* Notación

Usaremos la convención usual en probabilidad. 

** Variables aleatorias

Una variable aleatoria $X$ está definida a través de un ~espacio de probabilidad~ $(\mathcal{X}, \mathcal{F}, \pi)$. La función $\pi: \mathcal{F}\rightarrow[0,1]$  se llama la ~función de distribución~ de la variable aleatoria $X$. Escribimos $X \sim \pi$.

** Distribución paramétrica 

Decimos que una función de distribución es ~paramétrica~ si se puede identificar completamente la distribución con respecto a un ~vector de parámetros~ $\theta \in \mathbb{R}^p$. Esto lo denotamos de la siguiente manera

\begin{align}
\pi_\theta(x) \qquad \text{} \pi(x ; \theta)\,,
\end{align}

y si  $\theta \neq\theta'$ entonces $\pi_\theta(x) \neq \pi_{\theta'}(x)$ para cualquier $x$ en el soporte.

** Valores esperados

El ~valor esperado~ de una variable aleatoria $X \sim \pi$ se define como
\begin{align}
\mathbb{E}[X] = \int_{\mathcal{X}} x \, \pi(x) \, \text{d}x\,.
\end{align}

*** Abuso de notación
:PROPERTIES:
:reveal_background: #00468b
:END:

La ~función de densidad~ está definida como $\text{d}\pi/\text{d}x$. En la definición de valor esperado deberíamos de haber escrito $\text{d}\pi(x)$  o bien $\pi(\text{d}x)$ (integrales de Lebesgue). Pero para no ofuscar notación lo obviamos...

** Estadísticas de interés
La definición se puede extender con $f: \mathcal{X} \rightarrow \mathbb{R}$ y se calcula como
\begin{align}
\mathbb{E}[f(X)] = \int_{\mathcal{X}} f(x) \pi(x) \text{d}x\,.
\end{align}
#+REVEAL: split
Denotaremos de la siguiente manera
\begin{align}
\pi(f) := \mathbb{E}[f(X)]\,.
\end{align}

** Probabilidad condicional

La ~probabilidad condicional~ de $A$ dado el evento $B$ se denota $\pi(A|B)$ y está definida como
\begin{align}
\pi(A|B) = \frac{\pi(A \cap B)}{\pi(B)}
\end{align}

* Repaso

#+REVEAL: split

#+REVEAL: split
** Regla de Bayes

La ~regla de Bayes~ utiliza la definición de probabilidad condicional para hacer inferencia a través de 
\begin{align}
\pi(A|B) = \frac{\pi(B|A) \pi(A)}{\pi(B)}\,.
\end{align}
#+REVEAL: split

#+DOWNLOADED: screenshot @ 2022-01-21 20:44:26
#+caption: Portada de citep:Kruschke2014.
#+attr_html: :width 1200 :align center
[[file:images/20220121-204426_screenshot.png]]

** Ejemplos

#+ATTR_REVEAL: :frag (appear)
- Verosimilitud: $x |\theta \sim \mathsf{Binomial}(n, \theta)$ + Previa: $\theta \sim \mathsf{Beta}(\alpha, \beta)$ = Posterior: ?
- Verosimilitud: $x |\theta \sim \mathsf{Uniforme}(0, \theta)$ + Previa: $\theta \sim \mathsf{Pareto}(\theta_0)$ = Posterior: ?

* Motivación

Por medio de metodología Bayesiana podemos cuantificar incertidumbre en:
#+ATTR_REVEAL: :frag (appear)
- Observaciones. 
- Parámetros. 
- Estructura. 

#+REVEAL: split
  Es fácil especificar y ajustar modelos. Pero hay preguntas cuyas respuestas no han quedado claras:
#+ATTR_REVEAL: :frag (appear)
  1. Construcción. 
  2. Evaluación. 
  3. Uso.

  #+BEGIN_NOTES

  Programación probabilística. 
  
  #+END_NOTES


#+REVEAL: split
Los aspectos del flujo de trabajo Bayesiano consideran (citep:Gelman2020):
#+ATTR_REVEAL: :frag (appear)
1. Construcción iterativa de modelos. 
2. Validación de modelo (computacional).
3. Entendimiento de modelo. 
4. Evaluación de modelo.   

** Distinción importante

~Inferencia~ no es lo mismo que ~análisis de datos~ o que un ~flujo de trabajo~. 

#+BEGIN_NOTES

Inferencia es formular y calcular con probabilidades condicionales. 

#+END_NOTES

** ¿Por qué necesitamos un flujo de trabajo?

#+ATTR_REVEAL: :frag (appear)
- El cómputo puede ser complejo.
- Expandir nuestro entendimiento en aplicaciones.
- Entender la relación entre modelos.
- Distintos modelos pueden llegar a distintas conclusiones.

** Proceso iterativo

- La gente de ML sabe que el proceso de construcción de un modelo es iterativo, ¿por qué no utilizarlo?

#+BEGIN_NOTES

Una posible explicación puede encontrarse en citep:Gelman2021. El argumento es formal en cuanto a actualizar nuestras creencias como bayesianos. Sin embargo, con cuidado y un procedimiento científico puede resolver el asunto. 

#+END_NOTES


#+DOWNLOADED: screenshot @ 2022-01-21 23:09:51
#+caption: Tomado de citep:Gelman2020.
#+attr_html: :width 800 :align center
[[file:../images/20220121-230951_screenshot.png]]

# * Bibliografia                                                        :latex:

# bibliographystyle:abbrvnat 
# bibliography:references.bib
