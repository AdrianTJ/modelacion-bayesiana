#+TITLE: EST-46115: Modelación Bayesiana
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: MCMC
#+STARTUP: showall
:REVEAL_PROPERTIES:
#+LANGUAGE: es
#+OPTIONS: num:nil toc:nil timestamp:nil
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_THEME: night
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="Modelación Bayesiana">
#+REVEAL_INIT_OPTIONS: width:1600, height:900, margin:.2
#+REVEAL_EXTRA_CSS: ./mods.css
#+REVEAL_PLUGINS: (notes)
:END:
#+PROPERTY: header-args:R :session mcmc :exports both :results output org :tangle ../rscripts/02-mcmc.R :mkdirp yes :dir ../
#+EXCLUDE_TAGS: toc latex


#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Primavera, 2022.\\
*Objetivo*. Estudiar el método general de integración Monte Carlo vía cadenas de Markov (MCMC). La estrategia será construir poco a poco utilizando los principios básicos que lo componen. \\
*Lectura recomendada*: Capítulo 3 de citep:Reich2015. Capítulo 7 de citep:Dogucu2021. 
#+END_NOTES


* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introduccion][Introduccion]]
- [[#muestreo-por-aceptación-rechazo][Muestreo por aceptación rechazo]]
  - [[#implementación][Implementación]]
  - [[#tarea][Tarea]]
  - [[#propiedades][Propiedades]]
- [[#qué-hemos-visto][¿Qué hemos visto?]]
- [[#muestreo-por-cadenas-de-markov][Muestreo por Cadenas de Markov]]
  - [[#ejemplo][Ejemplo:]]
  - [[#pregunta][Pregunta]]
  - [[#modelación-del-tour-de-ventas][Modelación del tour de ventas]]
  - [[#conclusiones][Conclusiones]]
- [[#generalizando][Generalizando...]]
  - [[#pseudo-código][Pseudo-código]]
  - [[#desentrañando][Desentrañando]]
  - [[#implementación][Implementación]]
  - [[#tarea-1][Tarea (1)]]
  - [[#tarea-2][Tarea (2)]]
- [[#el-método-metropolis-hastings][El método Metropolis-Hastings]]
  - [[#tarea-3][Tarea (3)]]
- [[#referencias][Referencias]]
:END:



* Introduccion

#+begin_src R :exports none :results none

  ## Setup --------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)
  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 15))

  ## Cambia el número de decimales para mostrar
  options(digits = 2)

  sin_lineas <- theme(panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank())
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)

  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())

  #+end_src


El interés es poder resolver
\begin{align}
\mathbb{E}[f] = \int_{\Theta}^{} f(\theta) \, \pi(\theta | y ) \,  \text{d}\theta\,. 
\end{align}

Sin embargo, ~no podemos generar~ $\theta^{(i)} \overset{\mathsf{iid}}{\sim} \pi(\theta|y)$.


* Muestreo por aceptación rechazo

Podemos utilizar una versión estocástica de muestreo por importancia.


#+BEGIN_NOTES
Para muestrear de $\pi$ necesitamos utilizar una distribución sustituto (lo
mismo hicimos con muestreo por importancia). Sólo que ahora permitimos rechazar
muestras que no correspondan con las regiones de alta densidad de nuestra
distribución objetivo. El rechazo se realiza lanzando una moneda. La tasa de
éxito depende del qué tanto cubre nuestra distribución sustituto.
#+END_NOTES

#+begin_src R :exports none :results none
  ## Muestreo por aceptacion rechazo ---------------
#+end_src

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/rejection-sampling.jpeg :exports results :results output graphics file
  crea_mezcla <- function(weights){
    function(x){
      weights$w1 * dnorm(x, mean = -1.5, sd = .5) +
        weights$w2 * dnorm(x, mean = 1.5, sd = .7)
    }
  }

  objetivo <- crea_mezcla(list(w1 = .6, w2 = .4))

  tibble(x = seq(-5, 5, length.out = 100)) |>
    mutate(y = objetivo(x),
           aprox = 3.3 * dnorm(x, 0, sd = 2)) |>
    ggplot(aes(x,y)) +
    geom_area(fill = "lightblue") +
    geom_line(aes(x, aprox), lty = 2) +
    geom_ribbon(aes(ymin = y, ymax = aprox), fill = "salmon") + sin_lineas +
    sin_ejes

#+end_src
#+caption: Esquema de muestreo. 
#+RESULTS:
[[file:../images/rejection-sampling.jpeg]]


** Implementación
Necesitamos algunas cosas. Ser capaces de ~evaluar~ nuestra distribución
objetivo. Ser capaces de ~evaluar~ *y* ~muestrear~ de nuestra distribución de
muestreo.

#+REVEAL: split
#+caption: Distribución objetivo. 
#+begin_src R :exports code :results none
  crea_mezcla <- function(weights){
    function(x){
      weights$w1 * dnorm(x, mean = -1.5, sd = .5) +
        weights$w2 * dnorm(x, mean = 1.5, sd = .7)
    }
  }
  objetivo <- crea_mezcla(list(w1 = .6, w2 = .4))
  M        <- 3.3
#+end_src

#+REVEAL: split

El objetivo del curso *no* es programación orientada a objetos. Pero todxs lxs
jóvenes /cool/ lo utilizan.

#+REVEAL: split
#+caption: Distribución de muestreo. 
#+begin_src R :exports code :results none
  library(R6)
  ModeloNormal <-
    R6Class("ProbabilityModel",
            list(
              mean = NA,
              sd = NA,
              ## Inicializador
              initialize = function(mean = 0, sd = 1){
                self$mean = mean
                self$sd   = sd
              },
              ## Muestreador
              sample = function(n = 1){
                rnorm(n, self$mean, sd = self$sd)              
              },
              ## Evaluacion de densidad
              density = function(x, log = TRUE){
                dnorm(x, self$mean, sd = self$sd, log = log)
              }           
            ))
#+end_src

#+BEGIN_NOTES
En muestreo por aceptación rechazo necesitamos definir una distribución de la
cual *si podamos* generar números aleatorios. El inconveniente es, además, *conocer*
qué tanto podemos inflar la densidad de nuestra propuesta para /cubrir/ la
distribución objetivo.
#+END_NOTES

#+REVEAL: split
#+begin_src R :exports code :results none
  crea_rejection_sampling <- function(objetivo, aprox, M){
    function(niter){
      muestras <- matrix(nrow = niter, ncol = 3)
      for (ii in seq(1, niter)){
        propuesta <- aprox$sample()
        p <- objetivo(propuesta)
        g <- aprox$density(propuesta, log = FALSE)
        u <- runif(1)
        if (u < p/(M * g)) {  ## Aceptamos 
          muestras[ii, 2] <- 1
        } else {              ## Rechazamos 
          muestras[ii, 2] <- 0
        }
        muestras[ii, 1] <- propuesta
        muestras[ii, 3] <- u 
      }
      muestras 
    }
  }
#+end_src

#+REVEAL: split
#+HEADER: :width 1200 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/muestreo-aceptacion.jpeg  :exports results :results output graphics file
  modelo.muestreo  <- ModeloNormal$new(mean = 0, sd = 2)
  muestreo_rechazo <- crea_rejection_sampling(objetivo, modelo.muestreo, M)

  muestras <- muestreo_rechazo(5000) |>
    as.tibble() |>
    mutate(density = modelo.muestreo$density(V1, log = FALSE))

  g1 <- muestras |>
    ggplot(aes(V1, V3 * modelo.muestreo$density(V1, log = FALSE))) +
    geom_point(aes(color = factor(V2))) + sin_lineas + sin_ejes + sin_leyenda +
    xlab("") + ylab("") +
    ggtitle(paste("Muestras en el espacio (x,u), eficiencia: ", mean(muestras$V2)))

  g2 <- muestras |>
    filter(V2 == 1) |>
    ggplot(aes(V1)) +
    geom_histogram() + 
    sin_lineas + sin_ejes + sin_leyenda +
    xlab("") + ylab("") +
    ggtitle("Histograma de las muestras generadas")

  g1 + g2 
#+end_src

#+RESULTS:
[[file:../images/muestreo-aceptacion.jpeg]]

** Tarea

- ¿Qué pasa si $M$ es demasiado grande? Juega con el código e interpreta los resultados. 
- ¿Qué pasa si $M$ no es suficiente para cubrir la distribución objetivo? Juega con el código e interpreta los resultados.

** Propiedades

*Lemma (~Consistencia de muestreo por rechazo~)*. El método de muestreo de aceptación-rechazo genera muestras $x^{(i)}$ con $i = 1, \ldots, N$ que son independientes y distribuidas acorde a la distribución objetivo $\pi$.

/Prueba/ Usemos probabilidad condicional para medir
\begin{align}
\pi(x | \textsf{aceptar}) = \frac{\pi(\textsf{aceptar} | x) \times \pi(x)}{\pi(\textsf{aceptar})}\,.
\end{align}


* ¿Qué hemos visto?

- El método Monte Carlo se puede utilizar para aproximar integrales.
- Se puede utilizar una distribución sustituto para generar números aleatorios que nos interesan.
- Podemos lanzar monedas para /filtrar/ sólo los aleatorios que tengan altas probabilidades.
- Hemos utilizado el supuesto de independencia.


* Muestreo por Cadenas de Markov

Vamos a relajar el supuesto de independencia. Es decir, vamos a generar una
secuencia de números que /esperamos/ se comporte tal cómo nos interesa.

** Ejemplo:

#+DOWNLOADED: screenshot @ 2022-02-03 12:21:07
#+caption: Problema del café. 
#+attr_html: :width 700 :align center
[[file:images/20220203-122107_screenshot.png]]

#+REVEAL: split
El vendedor de galletas quiere satisfacer la demanda para acompañar un café. El vendedor:
- Viaja entre las islas.
- Decide si se queda o no se queda en la isla donde está. 
- Se puede mover entre islas contiguas (a través de puentes). 
- Tiene mala memoria y  pregunta el número de casas en las islas aledañas (todos los días).
- Quiere visitar todas las islas y vender galletas.
- Viaja en bicicleta. 


#+REVEAL: split
También es astuto. Sabe que en donde haya mucha gente venderá mas, pero también
sabe que una isla siempre lo podría llevar a una mas grande. Asi que a veces le
convendrá viajar a una isla pequeña. Asi que utilizará el ~principio de
aceptación rechazo~ para decidir si se moverá a la siguiente isla.

#+REVEAL: split
1. Lanza una moneda para decidir si se mueve a la izquierda o derecha.
2. Decide si se mueve de acuerdo al cociente de poblaciones.

** Pregunta

En el contexto de nuestro problema ¿qué cambiaría si tuviera conocimiento censal
del archipiélago y pudiera viajar en avión?

** Modelación del /tour/ de ventas

El vendedor se encuentra en el $t$ -ésimo día. Supongamos que va a evaluar si se
cambia a la isla de la derecha. Sea $\pi_\star$ la población de la isla propuesta y
$\pi_{t}$ la población de la isla actual. Entonces el vendedor cambia de isla
con probabilidad

$$\alpha_{\textsf{mover}}= \frac{\pi_\star}{\pi_{t}}\,.$$

#+BEGIN_NOTES
Entre mas parecidas sean las poblaciones de las islas mas *indeciso*
será de moverse. Por definición  $\alpha_{\textsf{mover}} \in (0,1)$. De
hecho, podemos definir la probabilidad de viajar a otra isla por medio de

$$\alpha(t, \star) = \min \Bigg\{ 1, \frac{\pi_\star}{\pi_{t}}\Bigg\},$$

pues incluye los dos casos. 
#+END_NOTES


#+REVEAL: split
El tiempo que un vendedor pase en cada isla corresponderá al tamaño relativo de
ellas.

#+begin_src R :exports none :results none
  ## Caminata entre islas --------------------------
  set.seed(1087)
#+end_src

#+begin_src R :exports code :results none
  islas <- tibble(islas = 1:5, pob = seq(60, 100, by = 10))
  camina_isla <- function(i){ # i: isla actual
    u_izq <- runif(1) # Lanzamos volado para ver si nos vamos izq o der. 
    v <- ifelse(u_izq < 0.5, i - 1, i + 1)  # Pedimos índice isla vecina. 
    if (v < 1 | v > 5) { # si estas en los extremos y el volado indica salir
      return(i)
    }
    u_cambio <- runif(1) # Moneda de aceptacion de cambio
    p_cambio = min(islas$pob[v]/islas$pob[i], 1)
    if (u_cambio < p_cambio) {
      return(v) # isla destino
    }
    else {
      return(i) # me quedo en la misma isla
    }
  }
#+end_src

#+REVEAL: split
#+begin_src R :exports none :results none
  pasos <- 100000; iteraciones <- numeric(pasos)
  iteraciones[1] <- sample(1:5, 1) # isla inicial
  for (j in 2:pasos) {
      iteraciones[j] <- camina_isla(iteraciones[j - 1])
  }
  caminata <- tibble(paso = 1:pasos, isla = iteraciones)
#+end_src

#+HEADER: :width 1200 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/caminata-cafe.jpeg :exports results :results output graphics file
  plot_caminata <- ggplot(caminata[1:500, ], aes(x = paso, y = isla)) +
    geom_point(size = 0.8) +
    geom_path(alpha = 0.5) +
    labs(title = "Caminata aleatoria") +
    scale_x_continuous(trans = "log10", "Tiempo", breaks = c(1, 2, 5, 20, 100, 500)) +
    scale_y_continuous( expression(theta)) + sin_lineas
  plot_dist <- ggplot(caminata, aes(x = isla)) +
    geom_bar(fill = "darkgray", aes(y = (..count..)/sum(..count..))) +
    geom_bar(data = islas |>  mutate(prop = pob/sum(pob)),
             aes(x = islas, y = prop), fill = "steelblue", alpha = .3, stat = "identity") + 
    scale_x_continuous(expression(theta), breaks = 1:10) +
    ylim(0,.5) + 
    labs(title = "Distribución objetivo (Histograma)", 
         y = expression(hat(pi)(theta))) + sin_lineas + coord_flip()
  plot_caminata + plot_dist
#+end_src
#+caption: Caminata aleatoria entre 5 islas. 
#+RESULTS:
[[file:../images/caminata-cafe.jpeg]]

#+begin_src R :exports none :results none :eval never
  ## Animación histograma -----------------------------------
  library(gganimate)
  res <- caminata |>
    mutate(tiempo = cut(paso, breaks = seq(0, n(), by = 10))) |>
    group_by(isla, tiempo) |>
    count() |>
    ungroup() |>
    complete(tiempo, nesting(isla), fill = list(n = 0)) |>
    group_by(isla) |>
    mutate(count = cumsum(n)) |>
    group_by(tiempo) |>
    mutate(prop = count/sum(count)) |>
    arrange(tiempo, isla) |>
    ungroup()


  anim <- res |>
    mutate(tiempo = as.numeric(tiempo)) |>
    filter(tiempo <= 1500) |>
    ggplot(aes(x = isla, y = prop)) +
    geom_bar(fill = "darkgray", stat = "identity") +
    coord_flip() + sin_lineas +
    geom_bar(data = islas |>  mutate(prop = pob/sum(pob)),
             aes(x = islas, y = prop), fill = "steelblue", alpha = .3, stat = "identity") + 
    scale_x_continuous(expression(theta), breaks = 1:10) +
    transition_states(tiempo, transition_length = 2, state_length = 1) +
    ease_aes("exponential-out")

  animate(anim, renderer = ffmpeg_renderer(), height = 300, width = 900)

  anim_save("./images/islas-histograma.mp4")

#+end_src

** Conclusiones

- La estrategia del vendedor le permitirá, en el ~largo plazo~,  visitar todas las islas.
- El tiempo que pasa en cada isla$^\dagger$ corresponde a la población relativa.
- Al principio, aún no representa dicha proporción.


* Generalizando... 

Supongamos que tenemos un modelo
\begin{gather}
Y| \mu \sim \mathsf{N}(\mu, 0.75^2)\,,\\
\mu \sim \mathsf{N}(0,1^2)\,.
\end{gather}

~Verifica~ que bajo la observación $y = 6.25$ la distribución posterior que nos interesa es
\begin{gather}
\mu | y \sim \mathsf{N}(4, 0.6^2)\,.
\end{gather}

#+REVEAL: split
~Vamos a suponer~ que *no* sabemos muestrear de una Normal. Asi que usaremos una
estrategia parecida que con el vendedor de galletas. La estrategia será:
1. Generar una propuesta $\mu_\star$ para cambiarnos de nuestro valor actual $\mu_t$.
2. Decidir si nos movemos utilizando un cociente que tome en cuenta los pesos relativos.

** Pseudo-código 
- [ ]  Vamos a proponer una ''moneda'' para lanzar la *dirección* de movimiento.
  Esto lo haremos con
  \begin{align}
  \mu_\star | \mu_t \sim \mathsf{Uniforme}( \mu_t - \omega, \mu_t + \omega)\,.
  \end{align}

#+REVEAL: split
- [ ] Vamos a decidir si nos movemos de acuerdo a los pesos relativos
  \begin{align}
  \alpha(\mu_t, \mu_\star)  = \min \left\lbrace1 , \frac{\pi(\mu_\star|y)}{\pi(\mu_t|y)} \right\rbrace\,.
  \end{align}
  
** Desentrañando

Escribamos el cociente en términos de la densidad de la distribución posterior y simplifiquemos. ¿Qué observas? 

** Implementación

Veamos cómo implementarlo. Vamos a suponer una distribución de muestreo con un intervalo de longitud 2. Es decir,  $\omega = 1$. 

#+begin_src R :exports none :results none
  ## Caminata en espacio continuo ------------------------
#+end_src

#+REVEAL: split
#+caption: Modelo de muestreo uniforme. 
#+begin_src R :exports code :results none
  ModeloUniforme <-
    R6Class("ProbabilityModel",
            list(
              a = NA,
              b = NA, 
              initialize = function(a = 0, b = 1){
                self$a = a
                self$b = b
              }, 
              sample = function(n = 1){
                runif(n, self$a, self$b)              
              },
              density = function(x, log = TRUE){
                dunif(x, self$a, self$b, log = log)
              }           
            ))
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none
  crea_cadena_markov <- function(objetivo, muestreo){
    function(niter){
      muestras <- matrix(nrow = niter, ncol = 2)
      ## Empezamos en algun lugar
      estado   <- muestreo$sample()
      muestras[1,1] <- estado
      muestras[1,2] <- 1
      for (ii in 2:niter){
        propuesta   <- estado + muestreo$sample()
        p_propuesta <- objetivo$density(propuesta, log = FALSE)
        p_estado    <- objetivo$density(estado, log = FALSE)
        if (runif(1) < p_propuesta/p_estado) {
          muestras[ii, 2] <- 1 ## Aceptamos
          muestras[ii, 1] <- propuesta
        } else {
          muestras[ii, 2] <- 0 ## Rechazamos
          muestras[ii, 1] <- estado
        }
        estado <- muestras[ii, 1]
      }
      colnames(muestras) <- c("value", "accept")
      muestras
    }
  }
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none
  objetivo <- ModeloNormal$new(mean = 4, sd = .6)
  muestreo <- ModeloUniforme$new(a = -1, b = 1)

  mcmc <- crea_cadena_markov(objetivo, muestreo)
  muestras <- mcmc(5000)
#+end_src

#+HEADER: :width 1200 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/primer-mcmc.jpeg :exports results :results output graphics file
  g1 <- muestras |>
    as.tibble() |>
    mutate(iter = 1:n()) |>
    ggplot(aes(iter, value)) +
    geom_line() + sin_lineas + 
    ggtitle(paste("Trayectoria, eficiencia: ", mean(muestras[,2])))

  g2 <- muestras |>
    as.tibble() |>
    ggplot(aes(value)) +
    geom_histogram(aes(y = ..density..)) +
    stat_function(fun = objetivo$density,
                  args = list(log = FALSE),
                  color = "salmon",
                  size = 2) + sin_lineas + 
    ggtitle("Histograma")

  g1 + g2
#+end_src
#+caption: Nuestra segunda cadena de Markov. 
#+RESULTS:
[[file:../images/primer-mcmc.jpeg]]

** Tarea (1)

Sin modificar el número de iteraciones, considera cambiar la dispersión de la distribución de muestreo.
- ¿Qué observas si $\omega = 0.01$?
- ¿Qué observas si $\omega = 100$?

** Tarea (2)

Regresa a nuestro ejemplo conjugado Beta-Binomial. Considera una previa $\theta \sim \mathsf{Beta}(2,3)$ y una verosimilitud $Y|\theta \sim \mathsf{Binomial}(2, \theta)$. Escribe la distribución posterior. 

#+REVEAL: split
Para este caso tenemos un ligero inconveniente. El soporte para $\theta$ es el intervalo cerrado $[0,1]$ y utilizar una propuesta como en el caso anterior nos podría colocar (casi seguramente) fuera del intervalo. Así que lo que haremos será un pequeña modificación a cómo generamos nuestra propuesta y cómo evaluamos la probabilidad de transición.

#+REVEAL: split
- [ ] Vamos a generar propuestas de la siguiente manera
  \begin{align}
  \theta_\star | \theta_t \sim \mathsf{Beta}(\alpha, \beta)\,.
  \end{align}
- [ ] Vamos a calcular la probabilidad de transición a través de
  \begin{align}
  \alpha(\theta_t, \theta_\star) = \min \left\lbrace 1,  \frac{\pi(\theta_\star|y)}{\pi(\theta_t|y)} \cdot \frac{g(\theta_t)}{g(\theta_\star)}\right\rbrace\,,
  \end{align}
  donde $g$ denota la densidad de la distribución de transición definida arriba.

#+REVEAL: split
Modifica el código de clase para implementar este muestreador. Compara con muestras exactas del modelo posterior. 

* El método Metropolis-Hastings 

La forma más general que tenemos para generar una cadena de muestras es el método de Metropolis-Hastings.
#+REVEAL: split

- [ ] Generamos propuestas en cada iteración por medio de 
  \begin{align}
  \theta_\star | \theta_t \sim q( \theta_\star | \theta_t )\,.
  \end{align}
- [ ] Calculamos la probabilidad de transición como 
  \begin{align}
  \alpha(\theta_t, \theta_\star) = \min \left\lbrace 1,  \frac{\pi(\theta_\star)}{\pi(\theta_t)} \cdot \frac{q(\theta_t|\theta_\star)}{q(\theta_\star|\theta_t)}\right\rbrace\,,
  \end{align}
  donde la notación hace énfasis en que este mecanismo puede generar muestras de la distribución $\pi$ utilizando un generador $q$.

** Tarea (3)

- [ ] Repasemos ~los métodos anteriores~.
- [ ] ¿Qué pasa si desconocemos la constante de normalización de la distribución objetivo?



* Referencias                                                         :latex:

bibliographystyle:abbrvnat
bibliography:references.bib


