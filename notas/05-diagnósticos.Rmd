
# Diagnósticos 


```{r, include=FALSE, message=FALSE}
library(tidymodels)
library(tidyverse)
library(cmdstanr)
library(rstanarm)
library(bayesplot)
library(loo)

library(patchwork)
library(scales)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning=FALSE, 
                      fig.align = 'center', fig.width = 5, fig.height=3, cache = TRUE)
comma <- function(x) format(x, digits = 2, big.mark = ",")
theme_set(theme_linedraw())
color.blues <- c(NA,"#BDD7E7", "#6BAED6", "#3182BD", "#08519C", "#074789", "#063e77", "#053464")
color.itam  <- c("#00362b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f")


sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
sin_leyenda <- theme(legend.position = "none")
sin_ejes <- theme(axis.ticks = element_blank(), 
                  axis.text = element_blank())
```

## Resiudales {-}

```{r}
kidiq <- read_csv("../datos/kidiq.csv")
kidiq %>% head()
```

```{r}
kidiq <- kidiq %>% mutate(mom_iq_c = mom_iq - mean(mom_iq))

fit_kid <- stan_glm(kid_score ~ mom_iq_c, data=kidiq, refresh = 0)
print(fit_kid)
```

```{r}

kidiq %>% 
    mutate( residuals = kid_score - predict(fit_kid)) %>% 
    ggplot(aes(x = mom_iq, y = residuals)) + 
        geom_point() + sin_lineas + 
        geom_hline(yintercept = 0, lty = 2, size = 1) + 
        # geom_hline(yintercept = c(18.3, -18.3), lty = 2) + 
        geom_ribbon(aes(ymin = -18.3, ymax = 18.3), alpha = .3) + 
        geom_ribbon(aes(ymin = -2 * 18.3, ymax = 2 * 18.3), alpha = .3) + 
        ggtitle("Residuales contra predictor")

```

```{r}

g1 <- kidiq %>% 
    mutate( residuals  = kid_score - predict(fit_kid), 
            prediccion = predict(fit_kid)) %>% 
    ggplot(aes(x = prediccion, y = residuals)) + 
        geom_point() + sin_lineas + 
        geom_hline(yintercept = 0, lty = 2, size = 1) + 
        # geom_hline(yintercept = c(18.3, -18.3), lty = 2) + 
        geom_ribbon(aes(ymin = -18.3, ymax = 18.3), alpha = .3) + 
        geom_ribbon(aes(ymin = -2 * 18.3, ymax = 2 * 18.3), alpha = .3) + 
        ggtitle("Residuales contra prediccion")
    
g2 <- kidiq %>% 
    mutate( residuals  = kid_score - predict(fit_kid), 
            prediccion = predict(fit_kid)) %>% 
    ggplot(aes(x = kid_score, y = residuals)) + 
        geom_point() + sin_lineas + 
        geom_hline(yintercept = 0, lty = 2, size = 1) + 
        # geom_hline(yintercept = c(18.3, -18.3), lty = 2) + 
        geom_ribbon(aes(ymin = -18.3, ymax = 18.3), alpha = .3) + 
        geom_ribbon(aes(ymin = -2 * 18.3, ymax = 2 * 18.3), alpha = .3) + 
        ggtitle("Residuales contra observación")

g1 + g2
```

```{r}

a <- 0.6    
b <- 86.8
sigma <- 18.3

kidiq_sim <- tibble(mom_iq_c    = kidiq$mom_iq_c, 
                     kid_score = a * kidiq$mom_iq_c + b + 18.3 * rnorm(nrow(kidiq)))

fit_sim <- stan_glm(kid_score ~ mom_iq_c, data=kidiq_sim, refresh = 0)
print(fit_sim)

```

```{r}
g1 <- kidiq_sim %>% 
    mutate( residuals  = kid_score - predict(fit_sim), 
            prediccion = predict(fit_sim)) %>% 
    ggplot(aes(x = prediccion, y = residuals)) + 
        geom_point() + sin_lineas + 
        geom_hline(yintercept = 0, lty = 2, size = 1) + 
        # geom_hline(yintercept = c(18.3, -18.3), lty = 2) + 
        geom_ribbon(aes(ymin = -18.3, ymax = 18.3), alpha = .3) + 
        geom_ribbon(aes(ymin = -2 * 18.3, ymax = 2 * 18.3), alpha = .3) + 
        ggtitle("Residuales contra prediccion")
    
g2 <- kidiq_sim %>% 
    mutate( residuals  = kid_score - predict(fit_sim), 
            prediccion = predict(fit_sim)) %>% 
    ggplot(aes(x = kid_score, y = residuals)) + 
        geom_point() + sin_lineas + 
        geom_hline(yintercept = 0, lty = 2, size = 1) + 
        # geom_hline(yintercept = c(18.3, -18.3), lty = 2) + 
        geom_ribbon(aes(ymin = -18.3, ymax = 18.3), alpha = .3) + 
        geom_ribbon(aes(ymin = -2 * 18.3, ymax = 2 * 18.3), alpha = .3) + 
        ggtitle("Residuales contra observación")

g1 + g2
```

## Evaluación de la predictiva posterior {-}

```{r}

newcomb <- read_table("../datos/newcomb")
newcomb %>% head()

```

```{r}

newcomb %>% 
    ggplot(aes(x = y)) + 
        geom_histogram() + sin_lineas

```

```{r}

fit_newc <- stan_glm(y ~ 1, data=newcomb, refresh=0)
fit_newc

```

```{r}

y_rep <- posterior_predict(fit_newc)

```

```{r}

ppc_hist(newcomb$y, y_rep[1:19, ], binwidth = 8) + sin_lineas

```

```{r}

ppc_dens_overlay(newcomb$y, y_rep[1:100, ]) + sin_lineas

```

```{r}

ppc_stat(newcomb$y, y_rep, stat = "min", binwidth = 2) + sin_lineas

```

```{r}

unemp <- read_table("../datos/unemployment")
unemp %>% head()

```

```{r}

unemp %>% 
    ggplot(aes(year, y)) + 
        geom_line() + sin_lineas + 
        ylab("Unemployment rate (%)")

```
```{r}

fit_lag <- stan_glm(y ~ y_lag, data=unemp %>% mutate(y_lag = lag(y)), refresh=0)
fit_lag

```

```{r}
y_rep <- posterior_predict(fit_lag)
y_rep <- cbind(unemp$y[1], y_rep)
n_sims <- nrow(y_rep)
```

```{r}

as_tibble(y_rep) %>% 
    mutate(sim = 1:n_sims) %>% 
    sample_n(15) %>% 
    pivot_longer(cols = V1:70) %>% 
    mutate(year = rep(unemp$year, 15)) %>% 
    ggplot(aes(x = year, y = value)) + 
        geom_line() + 
        facet_wrap(~sim, ncol = 5)

```

```{r}

test <- function (y){
  n <- length(y)
  y_lag <- c(NA, y[1:(n-1)])
  y_lag_2 <- c(NA, NA, y[1:(n-2)])
  return(sum(sign(y-y_lag) != sign(y_lag-y_lag_2), na.rm=TRUE))
}
test_y <- test(unemp$y)
test_rep <- apply(y_rep, 1, test)
print(mean(test_rep > test_y))

```

```{r}
print(quantile(test_rep, c(.1,.5,.9)))
```

```{r}
ppc_stat(y=unemp$y, yrep=y_rep, stat=test, binwidth = 1) + sin_lineas
```

## Desviación estándar de los residuales $\sigma$ y varianza explicada $R^2$ {-}

$$ \hat R = 1 - \frac{\hat \sigma2}{\sigma_y^2} \,.$$
```{r}

data <- tibble(x = 1:5 - 3, 
               y = c(1.7, 2.6, 2.5, 4.4, 3.8) - 3)

summary(ols <- lm(y ~ x, data))

```

```{r}


fit_bayes <- stan_glm(y ~ x, data = data,
  prior_intercept = normal(0, 0.2, autoscale = FALSE),
  prior = normal(1, 0.2, autoscale = FALSE),
  prior_aux = NULL,
  seed = 108727, refresh = 0
)

c(OLS   = var(predict(ols))/var(data$y), 
  Bayes = var(predict(fit_bayes))/var(data$y))

```

```{r}

bayesR2 <- bayes_R2(fit_bayes)

mcmc_hist(data.frame(bayesR2), binwidth=0.02)  +
    xlab('Bayesian R2') +
    geom_vline(xintercept=median(bayesR2)) + sin_lineas

```

```{r}

bayesR2 <- bayes_R2(fit_kid)

g1_kid <- mcmc_hist(data.frame(bayesR2), binwidth=0.01)  +
    xlab('Bayesian R2') +
    geom_vline(xintercept=median(bayesR2)) + sin_lineas + 
    xlim(0, .35) + ggtitle("Modelo regresión")

g1_kid

```

```{r}

n <- nrow(kidiq)
kidiqr <- kidiq
kidiqr$noise <- array(rnorm(5*n), c(n,5))

```

```{r}
fit_kid_noise <- stan_glm(kid_score ~ mom_hs + mom_iq_c + noise, data=kidiqr,
                   seed=108727, refresh=0)
print(fit_kid_noise)
```

```{r}

c(median(bayesR2), median(bayesR2n<-bayes_R2(fit_kid_noise)))

```

```{r}

g2_kid <- mcmc_hist(data.frame(bayesR2n), binwidth=0.01)  +
    xlab('Bayesian R2') +
    geom_vline(xintercept=median(bayesR2n)) + sin_lineas + xlim(0, .35) + 
    ggtitle("Modelo con malos predictores")

g1_kid / g2_kid


```

## Evaluación de modelos {-}

Uno de los objetivos en Estadística Bayesiana es la predicción: considerando una muestra inicial y una nueva observación, deseamos encontrar la distribución condicional de la nueva observación, dada la muestra. Recordemos que, a esa distribución le llamamos distribución posterior predictiva. Por otra parte, al emplear modelos distintos, tenemos interés en compararlos para elegir al mejor en alguna característica que nos interese. En este curso, la característica que nos interesa es la calidad de los pronósticos, por lo que es necesario contar con métricas que nos permitan comparar distintos modelos a través de comparar la calidad de sus pronósticos y entonces tener elementos para elegir al mejor o a los mejores modelos conforme a la métrica empleada. 

Considerando lo anterior, a continuación desarrollamos los siguientes puntos: 

+ Motivación,  
+ Log Densidad Predictiva, 
+ Algunos Conceptos de Teoría de la Información,
+ Devianza y Criterio de Información y
+ Lecciones aprendidas.

### Motivación

Hasta el momento hemos visto, en el contexto de estadística Bayesiana, modelos simplificados del tipo:

$$
y_i = \beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k + \epsilon.
$$
En los que asumimos que la relación entre $Y$ y $X$ es lineal y asignamos una distribución a priori sobre los coeficientes $\beta_i$, denotara por por $\pi(\beta_0, \ldots,\beta_k, \sigma^2|X,Y)$, y una distribución para el término de error. A la distribución de $Y$ la denotamos por $Y|\beta, X \sim \pi(Y|\beta, X)$. 

La pregunta que abordamos es: si tuviéramos que usar el modelo para realizar predicciones ¿cómo mediríamos su capacidad predictiva? Una posible respuesta viene dada por el Error Cuadrático Medio:

$$
\frac{1}{n}\sum_{i=1}^{n}(y_i -\mathop{\mathbb{E}}(y_i|x_i))^2.
$$
Que intenta cuantificar, con un solo número, el error cometido. El problema de este enfoque es que estamos ignorando la naturaleza probabilística de nuestras predicciones al describir una distribución de probabilidad con un solo número. Por ejemplo, estamos dejando de cuantificar la incertidumbre asociada a las predicciones y esa es información muy valiosa que no queremos perder, porque nos ayuda a medir la calidad de estas.

#### Ejemplo

Considere la Gráfica de abajo, en la que se muestran dos modelos hipotéticos, el modelo verde con media 0 y el rosa con media 10, y se desea hacer una predicción. Con fines ilustrativos, pensemos que conocemos el valor real, $y=5$ a predecir, resaltado por la línea punteada azul. Si la función resumen que se elige para ambas distribuciones predictivas son sus respectivas medias, entonces la predicción puntual de ambos modelos es 0 es 10. Note que, en ambos modelos, el error absoluto es 5, lo que nos podría llegar a concluir que son igual de buenos. Sin embargo, podemos argumentar que el modelo verde tiene un mejor desempeño: si construimos un intervalo de credibilidad al 95 por ciento para el modelo verde, este contendrá al valor verdadero, cosa que no ocurría con el rosa al construir su respectivo intervalo de credibilidad al 95 por ciento. 

El lector podrá imaginar que estandarizar por la varianza podría ser de ayuda. Sin embargo, esto tiene el inconveniente de no funcionar bien para distribuciones no simétricas, por lo que solo resolveríamos parcialmente el problema. 

Lo anterior, muestra la necesidad de contar con métricas que incorporen la incertidumbre de las predicciones que realizamos, que sean de utilidad de comprar distintos modelos y elegir el mejor o los mejores. Por ello, a continuación, se presentan distintas métricas en este sentido.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
set.seed(1)
datos <- data.frame(dens = c(rnorm(10000, 10,2), rnorm(10000, 0, 5))
                   , lines = rep(c("modelo 1", "modelo 2"), each = 10000))
ggplot(datos, aes(x = dens, fill = lines)) + 
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = 5, linetype="dotted", 
                color = "blue", size=1.5)+
  labs(title="Comparación entre dos modelos considerando \n incertidumbre",
        x ="Predicción", y = "Densidad") +
  theme(legend.position = "none")
```

### Log Densidad Predictiva

Las Reglas de Puntuación proporcionan medidas resumidas para la evaluación de pronósticos probabilísticos, mediante la asignación de una puntuación numérica basada en la distribución predictiva y en el evento o valor que se materializa [@scoringRules]. El papel de las reglas de puntuación es animar al evaluador a realizar evaluaciones cuidadosas y ser honesto.Existen varias Reglas de Puntuación, aquí nos concentramos en Logarítmica.

La Log Densidad Predictiva mide la capacidad predictiva de un modelo, entendida como una variable aleatoria, y tiene la ventaja que incorpora la incertidumbre. La Log Densidad Predictiva asociada a la densidad predictiva $\pi(Y|\theta)$ está dada por $\log \pi (Y|\theta)$.


#### Ejemplo

Considere que la distribución predictiva para $Y$, es una normal multivarida con vector de medias $\mu$ y matriz de varianza - covarianza $\Sigma^2$, es decir $Y\sim N(\mu, \Sigma^2)$. Entonces la Densidad Predictiva está dada por:

$$
\pi(Y|\theta) = (2\pi)^{-k/2}|\Sigma|^{-1/2}\text{exp}\Big\{-\frac{1}{2}||Y-\mu||^2_{\Sigma}\Big\}.
$$

Y la correspondiente Log Densidad Predictiva es:
$$
\log \pi(Y|\theta) = -\frac{k}{2}\log(2\pi)-\frac{1}{2}|\Sigma|-\frac{1}{2}||Y-\mu||^2_{\Sigma}
$$
con 
$$
||Y-\mu||^2_{\Sigma} = (Y-\mu)^T\Sigma^{-1}(Y-\mu).
$$

Recordemos que, en el contexto de Bayesiano, contamos con todos los elementos para marginalizar sobre los parámetros, por lo que la Log Densidad Predictiva se puede escribir como:

\begin{equation}
\begin{split}
\log \pi_{post}(y_{i})&=\log\int\pi(y_{i}|\theta)\pi(\theta)\,\text{d}\theta\\
&=\log \mathbb{E}_{post }[\pi_{post}(y_{i})|\theta)]\\
\end{split}
\end{equation}

Con $\pi_{post}(\theta) = \pi(\theta|\underline{y}_n)$.

A la expresión anterior, le llamamos **Log  Densidad Predictiva Inducida por el Modelo Posterior**. Sin embargo, presenta retos en su cálculo. Recordemos que el objetivo es evaluar la calidad de las predicciones, entonces los retos son:

1. ¿Qué pasa si tenemos nuevos datos, cómo incorporamos la nueva información para comparar los modelos?
2. ¿Qué pasa si no tenemos nuevos datos, entonces cómo comparamos la capacidad predictiva de los modelos?
3. ¿Qué pasa si no podemos conocer la Distribución Posterior, debido a que es complicado conocer la constante de normalización?

Abordemos la primera pregunta. Consideremos una nueva muestra $\tilde{y}_{1}, \ldots, \tilde{y}_m$ (una muestra futura o una submuestra que excluimos a propósito en  nuestra muestra inicial), tal que $\tilde{y}_i\sim f$. Entonces, podemos tomar la Log Densidad Predictiva Inducida por el Modelo Posterior y marginalizar respecto a distintas realizaciones de $y_i$, considerando $f$:

\begin{equation}
\begin{split}
\text{ELPD} &= \mathbb{E}_{f}[\log \pi_{post}(y_{i})]\\
&=\int\log \pi_{post}(\tilde{y}_{i})f(\tilde{y_i})\,\text{d}\tilde{y_i}.
\end{split}
\end{equation}

Donde ELPPD son las siglas en inglés de **Valor Esperado de la Log Densidad Predictiva**.

El problema es que $f$ es desconocida por lo que no podemos calcular la expresión anterior. Una solución es aproximarla, de esta forma, tenemos el **Valor Esperado de la Log Densidad Predictiva de Manera Puntual (ELLPD)**:

$$
\text{ELPPD} =\sum_i \mathbb{E}_{f}[\log \pi_{post}(\tilde{y}_{i})].
$$
Ahora abordemos la segunda pregunta. La situación es la siguiente: queremos evaluar la capacidad predictiva de nuestros modelos, pero no contamos con nuevas observaciones, esta situación se puede dar por distintas situaciones: resulta muy costoso, requiere mucho tiempo, no es factible repetir el experimento, etc. En ese caso, se puede usar la muestra original $y_1,\ldots, y_n$: 

\begin{equation}
\begin{split}
\log \pi_{post}(\underline{y}_n)&=\Pi_{i=1}^{n}\pi_{post}(Y_{i})\\
\text{LPPD} &=\sum_{i=1}^{n}\log \pi_{post}(y_i).
\end{split}
\end{equation}

A la expresión anterior se le llama **Log Densidad Predictiva Puntual**.

Ahora abordemos la tercera pregunta. No olvidemos que muchas veces la integral
$$
\pi_{post}(y_i) = \int \pi(y_i|\theta)\pi_{post}(\theta)\,\text{d}\theta
$$
resulta muy difícil de calcular. Por lo que la aproximamos con algún método Montecarlo:

$$
\frac{1}{s}\sum_{i=1}^{s}pi(y_i|\theta^s)
$$
con $\theta^s \sim \pi_{post}(\theta)$ y $S$ el número de simulaciones de la Distribución Posterior.

Cuando se emplea la aproximación de Monte Carlo, a la siguiente expresión le llamamos **LPPD - calculada**.

$$
\text{LPPD - calculada}=\sum_{i=1}^{n}\log\Big(\frac{1}{s}\sum_{s=1}^{S}\pi_{post}(y_i|\theta^s)\Big)
$$

Y es sobre esta última cantidad, sobre las que se construyen las métricas de evaluación de un modelo Bayesiano.  

### Algunos conceptos de Teoría de la Información 

Hemos hablado intuitivamente de incertidumbre, a continuación, formalizamos este concepto en el contexto de Teoría de la Información y lo usamos para introducir nuevos conceptos que, nos servirán para discutir el concepto de Devianza.

Intuitivamente entendemos la incertidumbre como sorpresa o asombro. Considere un evento cuya probabilidad de ocurrencia sea muy pequeña y que ocurra, en ese caso estaríamos muy sorprendidos. Por el contrario, la ocurrencia de un evento con alta probabilidad no nos sorprendería. Y la ocurrencia de eventos poso probables deberían sorprendernos más que los eventos más probables. A continuación, formalizamos esta idea mediante una función. 

La función que mide la incertidumbre asociada a una variable aleatoria $Y$, cuando toma un valor específico $y_i$, basados en su probabilidad de ocurrencia $P(y_i)=p_i$, debe de cumplir las siguientes características:

1. Incertidumbre alta cuando ocurra un evento con probabilidad muy baja. 
2. Incertidumbre baja cuando ocurra un evento con probabilidad muy alta.
3. Que la regla sea monótona, en el sentido que la ocurrencia de eventos con probabilidad baja tenga incertidumbre más alta que los eventos con probabilidad alta.

Las tres características anteriores las podemos obtener con la expresión $\log\frac{1}{p_i}$, que también se puede escribir como $-\log p_i$, que es la definición de incertidumbre para la variable aleatoria $Y$ cuando se observa $y_i$ cuya probabilidad asociada es $p_i$. A partir de este concepto, podemos definir tres conceptos fundamentales en Teoría de la Información. 

Considere dos modelos representados por las distribuciones de probabilidad $P$ y $Q$:

1. **Entropía**, mide el valor esperado de la incertidumbre, la denotamos por $H(P)$, y está dada por: 

\begin{equation}
\begin{split}
H(P) &= -\mathbb{E}(\log p_i) \\
&= -\sum_{i} p_i\log p_i
\end{split}
\end{equation}

2. **Entropía Cruzada**: $H(P,Q) = -\sum p_i\log q_i$

Note que:
$$
H(P,Q) \geq H(P,P)=H(P)
$$

3. **Entropía Relativa (Divergencia de Kullback-Leibler)**: $D_{KL} = H(P,Q)-H(p)$ que, en el contexto de Inferencia Bayesiana se conoce como la divergencia de Kullback-Leibler. Y la interpretamos como la similitud entre dos distribuciones de probabilidad $P$ y $Q$. 


Considere dos modelos cuyas distribuciones son $R$ y $Q$ que, aproximan el proceso generador de datos $P$. Entonces, podemos emplear la Entropía Cruzada para saber qué modelo tiene la mayor similitud al proceso generador de datos:

\begin{equation}
\begin{split}
D_{KL}(P||Q) - D(P||R)& = H(P,Q) - H(P,R)\\
&= -\mathbb{E}[\log q_i] + \mathbb{E}[\log r_i]
\end{split}
\end{equation}

Por lo que:

+ si $\mathbb{E}[\log q_i] < \mathbb{E}[\log r_i]$ preferimos el modelo $Q$, 
+ si $\mathbb{E}[\log q_i] > \mathbb{E}[\log r_i]$ preferimos el modelo $R$ y 
+ si $\mathbb{E}[\log q_i] = \mathbb{E}[\log r_i]$ somos indiferentes entre los modelos.

### Devianza y Criterio de Información 

Definimos la Devianza de la distribución $\pi$, y la denotamos por $D(\pi)$, como:

$$
D(\pi)=-2\log \pi(y | \theta)
$$
Dado la Entropía Relativa, la Devianza de un modelo, solo tiene sentido cuando se compara con otro modelo, por sí sola no indica nada. De esta manera, la podemos interpretar como una métrica que evalúa la capacidad de predictiva de una variable aleatoria para predecir eventos que no conocemos.

Una medida relacionada con la Devianza es el Criterio de Akaike, dado por:

$$
\tilde{D}(\pi)=-2\log(\pi(y|\theta)) + 2k.
$$
Donde $k$ es la dimensión en donde vive el vector aleatorio asociado a $\pi$. El criterio de Akaike penaliza a la Devianza de modelos que "son grandes" (k grandes).

#### Ejemplo

La Devianza de una Distribución Gaussiana está dada por:

$$
D(\pi(y|\mu, \Sigma)) = kc + ||y-\mu||^2_{\sigma}+log|\Sigma|
$$

### Lecciones Aprendidas

Contar con métricas que nos permitan evaluar las predicciones y que consideren la incertidumbre asociada, es algo sumamente útil para comparar modelos y elegir los mejores. Hasta el momento, respecto a métricas, hemos aprendimos lo siguiente:

1. Conceptos relacionados con la Log Densidad Predictiva
    i) Log Densidad Predictiva Inducida por el Modelo Posterior que, obtenemos cuando marginalizamos sobre los parámetros.
    ii) Valor Esperado de la Log Densidad Predictiva (ELPD) que, obtenemos cuando marginalizamos sobre nuevas realizaciones.
    iii) Valor Esperado de la Log Densidad Predictiva de Manera Puntual (ELPPD) que, es la aproximación de ELPD empleando nuevos datos.
    iv) Log Densidad Predictiva Puntual, que es la aproximación de ELPD considerando la muestra inicial. 
    v) LPPD - calculada que resulta de aproximar la LPPD aproximando la integral de la posteriori con algún método Montecarlo.

2. También introducimos conceptos de Teoría de la Información

    i) Entropía
    ii) Entropía Cruzada
    iii) Entropía Relativa
    
3. Devianza

4. Criterio de Información de Akaike.
    

Algo sumamente relevante es que, muchas veces tener nuevos datos para evaluar nuestros modelos, es algo que, por cuestiones de recursos, tiempo u otro tipo de costos no es posible. En ese caso, evaluamos las métricas en la  muestra inicial. Para ello, será necesario usar validación cruzada que, es el tema que veremos en a continuación.



```{r}
SEED <- 2141
set.seed(SEED) 

x <- 1:20
n <- length(x)
a <- 0.2
b <- 0.3
sigma <- 1
y <- a + b*x + sigma*rnorm(n)
fake <- data.frame(x, y)

head(fake)

```
Ajustamos modelo lineal

```{r}

fit_all <- stan_glm(y ~ x, data = fake, seed=SEED, chains=10, refresh=0)
print(fit_all)

```

Ajustamos modelo sin la observación 18

```{r}

fit_minus_18 <- stan_glm(y ~ x, data = fake[-18,], seed=SEED, refresh=0)
print(fit_minus_18)

```
Extraemos muestras de la posterior

```{r}
# Modelo completo
sims <- as.matrix(fit_all)

# Modelo sin observación
sims_minus_18 <- as.matrix(fit_minus_18)

```

Calculamos la distribución predictiva posterior para $x = 18$

```{r}

predpost <- tibble(y = seq(0,9,length.out=100)) %>% 
  mutate(x = map(y, ~mean(dnorm(., sims[,1] + sims[,2] * 18, sims[,3])*6+18))) %>% 
  unnest(x)

```

Calculamos la predictiva posterior (LOO) para $x = 18$

```{r}

predpost.loo <- tibble(y = seq(0,9,length.out=100)) %>% 
  mutate(x = map(y, ~mean(dnorm(., sims_minus_18[,1] + sims_minus_18[,2] * 18, 
                                sims_minus_18[,3])*6+18))) %>% 
  unnest(x)

```

Graficamos 

```{r}

p.datos <- ggplot(fake, aes(x = x, y = y)) +
  geom_point(color = "white", size = 3) +
  geom_point(color = "black", size = 2) + sin_lineas

p.modelo <- p.datos +
  geom_abline(
    intercept = mean(sims[, 1]),
    slope = mean(sims[, 2]),
    size = 1,
    color = "black"
  )

p.predpost <- p.modelo + 
  geom_path(data=predpost,aes(x=x,y=y), color="black") +
  geom_vline(xintercept=18, linetype=3, color="grey")

```

Agregamos la predicción con modelo incompleto (LOO)

```{r}

p.predloo <- p.predpost +
  geom_point(data=fake[18,], color = "grey50", size = 5, shape=1) +
  geom_abline(
    intercept = mean(sims_minus_18[, 1]),
    slope = mean(sims_minus_18[, 2]),
    size = 1,
    color = "grey50",
    linetype=2
  ) +
  geom_path(data=predpost.loo,aes(x=x,y=y), color="grey50", linetype=2)

p.predloo

```

Calculamos los residuales para ambos modelos. La función `loo_predict` calcula 
de manera agilizada las predicciones para validación utilizando LOO. 

```{r}

fake$residual <- fake$y-fit_all$fitted
fake$looresidual <- fake$y-loo_predict(fit_all)$value

```

```{r}

p1 <- ggplot(fake, aes(x = x, y = residual)) +
  geom_point(color = "black", size = 2, shape=16) +
  geom_point(aes(y=looresidual), color = "grey50", size = 2, shape=1) +
  geom_segment(aes(xend=x, y=residual, yend=looresidual)) +
  geom_hline(yintercept=0, linetype=2) + sin_lineas

p1

```


```{r }

c(posterior = round(sd(fake$residual),2), 
  loo       = round(sd(fake$looresidual),2), 
  sigma     = sigma)

```

Calculamos la log-densidad predictiva para cada simulación de nuestro modelo

$$\log \pi( \,y_i \,| x_i, \theta^s \, ), \, \qquad s = 1, \ldots, 10,000\,.$$

```{r }

ll_1 <- log_lik(fit_all)

```

Calculamos la log-densidad predictiva marginalizada para cada observación

$$\log \pi(y_i \, |\, x_i) = \log \left(\frac1S \sum_{s = 1}^S \pi( \,y_i \,| x_i, \theta^s \, ) \right) \,.$$

```{r }

fake$lpd_post <- matrixStats::colLogSumExps(ll_1) - log(nrow(ll_1))

```

Calculamos de manera puntual cada log-densidad predictiva sin usar la observación 
$i$-ésima

$$\log \pi(y_i \, |\, x_i) = \log \left(\frac1S \sum_{s = 1}^S \pi( \,y_i \,| x_i, \theta^s_{-i} \, ) \right) \,.$$

```{r }
loo_1 <- loo(fit_all)
fake$lpd_loo <- loo_1$pointwise[,"elpd_loo"] 
```

```{r }
p1 <- ggplot(fake, aes(x = x, y = lpd_post)) +
  geom_point(color = "black", size = 2, shape=16) +
  geom_point(aes(y=lpd_loo), color = "grey50", size = 2, shape=1) +
  geom_segment(aes(xend=x, y=lpd_post, yend=lpd_loo)) +
  ylab("log predictive density") + sin_lineas

p1
```

## Criterios de información y desempeño de modelos {-}

```{r }
fit_kid_m <- stan_glm(kid_score ~ mom_hs + mom_iq_c, data=kidiq,
                  seed=108727, refresh = 0)
fit_kid_m
```
```{r }
fit_kid_hs <- stan_glm(kid_score ~ mom_hs, data=kidiq,
                  seed=108727, refresh = 0)
fit_kid_hs
```
```{r}
waic.hs <- waic(fit_kid_hs)
waic.hs
```


```{r}
waic.m  <- waic(fit_kid_m)
waic.m
```

```{r}

loo_compare(waic.hs, waic.m)

```

```{r}

loo.hs <- loo(fit_kid_hs)
loo.hs

```

```{r}

loo.m  <- loo(fit_kid_m)
loo.m

```

```{r}

loo_compare(loo.hs, loo.m)

```

### Considerando un modelo mas complejo {-}

```{r}

fit_kid_int  <- stan_glm(kid_score ~ mom_hs + mom_iq_c + mom_hs:mom_iq_c,
                  data=kidiq, refresh=0)
fit_kid_int

```

```{r}

loo.int <- loo(fit_kid_int)
loo.int

```

```{r}

loo_compare(loo.m, loo.int, loo.hs)

```

```{r}
fit_kid_noise
loo.noise <- loo(fit_kid_noise)
loo_compare(loo.m, loo.int, loo.hs, loo.noise)

```

```{r}

fit_kid_cnoise <- stan_glm(kid_score ~ noise, data=kidiqr,
                  seed=108727, refresh = 0)
fit_kid_cnoise
loo.cnoise <- loo(fit_kid_cnoise)

```

```{r}

loo_compare(loo.m, loo.int, loo.hs, loo.noise, loo.cnoise)

```

```{r}

fit_hs_reg <- stan_glm(kid_score ~ mom_hs + mom_iq_c, prior=hs(), data=kidiq,
                     seed=SEED, refresh = 0)

fit_noise_reg <- stan_glm(kid_score ~ mom_hs + mom_iq_c + noise, prior=hs(),
                      data=kidiqr, seed=SEED, refresh = 0)

print(fit_hs_reg)
print(fit_noise_reg)

loo.hs_reg <- loo(fit_hs_reg)
loo.noise_reg <- loo(fit_noise_reg)

```

```{r}

loo_compare(loo.m, loo.int, loo.hs, loo.noise, loo.cnoise, loo.hs_reg, loo.noise_reg)

```

## Validación cruzada {-}

```{r}

loo(fit_kid_int)

kfold_10 <- kfold(fit_kid_int, K=10)

print(kfold_10)

```

```{r}

loo(fit_all)

```

